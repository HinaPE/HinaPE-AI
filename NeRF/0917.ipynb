{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# NeRF: Neural Radiance Fields",
   "id": "9534f501770791a5"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-23T03:01:49.244612Z",
     "start_time": "2024-09-23T03:01:49.241140Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import imageio.v2 as imageio\n",
    "import cv2\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from tqdm import trange"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T03:01:49.414875Z",
     "start_time": "2024-09-23T03:01:49.409055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "args = {\n",
    "    'config': 'configs/lego.txt',\n",
    "    'expname': 'blender_paper_lego',\n",
    "    'basedir': './logs',\n",
    "    'datadir': './data/nerf_synthetic/lego',\n",
    "    'netdepth': 8,\n",
    "    'netwidth': 256,\n",
    "    'netdepth_fine': 8,\n",
    "    'netwidth_fine': 256,\n",
    "    'N_rand': 1024,\n",
    "    'lrate': 0.0005,\n",
    "    'lrate_decay': 500,\n",
    "    'chunk': 32768,\n",
    "    'netchunk': 65536,\n",
    "    'no_batching': True,\n",
    "    'no_reload': False,\n",
    "    'ft_path': None,\n",
    "    'N_samples': 64,\n",
    "    'N_importance': 128,\n",
    "    'perturb': 1.0,\n",
    "    'use_viewdirs': True,\n",
    "    'i_embed': 0,\n",
    "    'multires': 10,\n",
    "    'multires_views': 4,\n",
    "    'raw_noise_std': 0.0,\n",
    "    'render_only': False,\n",
    "    'render_test': False,\n",
    "    'render_factor': 0,\n",
    "    'precrop_iters': 500,\n",
    "    'precrop_frac': 0.5,\n",
    "    'dataset_type': 'blender',\n",
    "    'testskip': 8,\n",
    "    'shape': 'greek',\n",
    "    'white_bkgd': True,\n",
    "    'half_res': True,\n",
    "    'factor': 8,\n",
    "    'no_ndc': False,\n",
    "    'lindisp': False,\n",
    "    'spherify': False,\n",
    "    'llffhold': 8,\n",
    "    'i_print': 100,\n",
    "    'i_img': 500,\n",
    "    'i_weights': 10000,\n",
    "    'i_testset': 50000,\n",
    "    'i_video': 50000\n",
    "}\n",
    "args = argparse.Namespace(**args)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_device('cuda')\n",
    "torch.set_default_dtype(torch.float32)"
   ],
   "id": "fd66c4dd127ffdb8",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T03:01:49.430081Z",
     "start_time": "2024-09-23T03:01:49.418974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trans_t = lambda t: torch.tensor([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, t],\n",
    "    [0, 0, 0, 1]], dtype=torch.float32)\n",
    "\n",
    "rot_phi = lambda phi: torch.tensor([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, np.cos(phi), -np.sin(phi), 0],\n",
    "    [0, np.sin(phi), np.cos(phi), 0],\n",
    "    [0, 0, 0, 1]], dtype=torch.float32)\n",
    "\n",
    "rot_theta = lambda th: torch.tensor([\n",
    "    [np.cos(th), 0, -np.sin(th), 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [np.sin(th), 0, np.cos(th), 0],\n",
    "    [0, 0, 0, 1]], dtype=torch.float32)\n",
    "\n",
    "\n",
    "def pose_spherical(theta, phi, radius):\n",
    "    c2w = trans_t(radius)\n",
    "    c2w = rot_phi(phi / 180. * np.pi) @ c2w\n",
    "    c2w = rot_theta(theta / 180. * np.pi) @ c2w\n",
    "    c2w = torch.tensor(np.array([[-1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]]), dtype=torch.float32) @ c2w\n",
    "    return c2w\n",
    "\n",
    "\n",
    "def get_embedder(multires: int):\n",
    "    include_input = True\n",
    "    input_dims = 3\n",
    "    max_freq_log2 = multires - 1\n",
    "    num_freqs = multires\n",
    "    log_sampling = True\n",
    "    periodic_fns = [torch.sin, torch.cos]\n",
    "\n",
    "    embed_fns = []\n",
    "    out_dim = 0\n",
    "    if include_input:\n",
    "        embed_fns.append(lambda x: x)\n",
    "        out_dim += input_dims\n",
    "    if log_sampling:\n",
    "        freq_bands = 2. ** torch.linspace(0., max_freq_log2, steps=num_freqs)\n",
    "    else:\n",
    "        freq_bands = torch.linspace(2. ** 0., 2. ** max_freq_log2, steps=num_freqs)\n",
    "\n",
    "    for _freq in freq_bands:\n",
    "        for _p_fn in periodic_fns:\n",
    "            embed_fns.append(lambda x, p_fn=_p_fn, freq=_freq: p_fn(x * freq))\n",
    "            out_dim += input_dims\n",
    "\n",
    "    return lambda x: torch.cat([fn(x) for fn in embed_fns], -1), out_dim\n",
    "\n",
    "\n",
    "def get_rays(H, W, K, pose):\n",
    "    i, j = torch.meshgrid(torch.linspace(0, W - 1, W),\n",
    "                          torch.linspace(0, H - 1, H), indexing='ij')  # pytorch's meshgrid has indexing='ij'\n",
    "    i = i.t()\n",
    "    j = j.t()\n",
    "    dirs = torch.stack([(i - K[0][2]) / K[0][0], -(j - K[1][2]) / K[1][1], -torch.ones_like(i)], -1)\n",
    "    # Rotate ray directions from camera frame to the world frame\n",
    "    rays_d = torch.sum(dirs[..., np.newaxis, :] * pose[:3, :3],\n",
    "                       -1)  # dot product, equals to: [c2w.dot(dir) for dir in dirs]\n",
    "    # Translate camera frame's origin to the world frame. It is the origin of all rays.\n",
    "    rays_o = pose[:3, -1].expand(rays_d.shape)\n",
    "    return rays_o, rays_d\n",
    "\n",
    "\n",
    "def batchify(fn, chunk):\n",
    "    if chunk is None:\n",
    "        return fn\n",
    "\n",
    "    def ret(inputs):\n",
    "        return torch.cat([fn(inputs[i:i + chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def run_network(inputs: torch.Tensor, viewdirs: torch.Tensor, network_fn, embed_fn, embeddirs_fn, netchunk: int):\n",
    "    inputs_flat = torch.reshape(inputs, [-1, inputs.shape[-1]])\n",
    "    embedded = embed_fn(inputs_flat)\n",
    "\n",
    "    if viewdirs is not None:\n",
    "        input_dirs = viewdirs[:, None].expand(inputs.shape)\n",
    "        input_dirs_flat = torch.reshape(input_dirs, [-1, input_dirs.shape[-1]])\n",
    "        embedded_dirs = embeddirs_fn(input_dirs_flat)\n",
    "        embedded = torch.cat([embedded, embedded_dirs], -1)\n",
    "\n",
    "    outputs_flat = batchify(network_fn, netchunk)(embedded)\n",
    "    outputs = torch.reshape(outputs_flat, list(inputs.shape[:-1]) + [outputs_flat.shape[-1]])\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def render_rays(rays_flat: torch.Tensor):\n",
    "    pass\n",
    "\n",
    "\n",
    "def batchify_rays(rays_flat: torch.Tensor, chunk: int):\n",
    "    all_ret = {}\n",
    "    for i in range(0, rays_flat.shape[0], chunk):\n",
    "        pass\n",
    "    return all_ret"
   ],
   "id": "95e32b1e2d8b3e4a",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup Log System",
   "id": "556b0a4394f7d07e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T03:01:49.601956Z",
     "start_time": "2024-09-23T03:01:49.597027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.makedirs(os.path.normpath(os.path.join(args.basedir, args.expname)), exist_ok=True)\n",
    "with open(os.path.normpath(os.path.join(args.basedir, args.expname, 'args.txt')), 'w') as file:\n",
    "    for arg in sorted(vars(args)):\n",
    "        attr = getattr(args, arg)\n",
    "        file.write('{} = {}\\n'.format(arg, attr))\n",
    "if args.config is not None:\n",
    "    with open(os.path.normpath(os.path.join(args.basedir, args.expname, 'config.txt')), 'w') as file:\n",
    "        file.write(open(args.config, 'r').read())"
   ],
   "id": "f061208b3994f8bd",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Images and Poses",
   "id": "cefec6ada029e44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T03:01:54.819082Z",
     "start_time": "2024-09-23T03:01:49.746978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metas = {}\n",
    "for _ in ['train', 'val', 'test']:\n",
    "    with open(os.path.normpath(os.path.join(args.datadir, 'transforms_{}.json'.format(_))), 'r') as fp:\n",
    "        metas[_] = json.load(fp)\n",
    "\n",
    "_images = []\n",
    "_poses = []\n",
    "_counts = [0]\n",
    "for _ in ['train', 'val', 'test']:\n",
    "    _img_array = []\n",
    "    _pose_array = []\n",
    "    for frame in metas[_]['frames'][::(1 if _ == 'train' else args.testskip)]:\n",
    "        _img_array.append(\n",
    "            imageio.imread(os.path.normpath(os.path.join(args.datadir, frame['file_path'] + '.png'))))\n",
    "        _pose_array.append(np.array(frame['transform_matrix']))\n",
    "    _counts.append(_counts[-1] + len(_img_array))\n",
    "    _images.append((np.array(_img_array) / 255.).astype(np.float32))\n",
    "    _poses.append(np.array(_pose_array).astype(np.float32))\n",
    "\n",
    "images_concatenated = np.concatenate(_images, 0)\n",
    "poses_concatenated = np.concatenate(_poses, 0)\n",
    "\n",
    "width, height = images_concatenated.shape[1:3]\n",
    "near, far = 2., 6.\n",
    "focal = .5 * width / np.tan(.5 * float(metas['train']['camera_angle_x']))\n",
    "i_train, i_val, i_test = [np.arange(_counts[i], _counts[i + 1]) for i in range(3)]\n",
    "K = np.array([\n",
    "    [focal, 0, 0.5 * width],\n",
    "    [0, focal, 0.5 * height],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "if args.half_res:\n",
    "    width = width // 2\n",
    "    height = height // 2\n",
    "    focal = focal / 2.\n",
    "\n",
    "    _ = np.zeros((images_concatenated.shape[0], height, width, 4))\n",
    "    for i, img in enumerate(images_concatenated):\n",
    "        _[i] = cv2.resize(img, (width, height), interpolation=cv2.INTER_AREA)\n",
    "    images_concatenated = _\n",
    "\n",
    "if args.white_bkgd:\n",
    "    images_concatenated = images_concatenated[..., :3] * images_concatenated[..., -1:] + (\n",
    "            1. - images_concatenated[..., -1:])\n",
    "else:\n",
    "    images_concatenated = images_concatenated[..., :3]\n",
    "\n",
    "render_poses = torch.stack([pose_spherical(angle, -30.0, 4.0) for angle in np.linspace(-180, 180, 40 + 1)[:-1]], 0)\n",
    "if args.render_test:\n",
    "    render_poses = np.array(poses_concatenated[i_test])"
   ],
   "id": "197e717d5937a795",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create NeRF Model ",
   "id": "bf1440dd2bbec707"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T03:01:54.992Z",
     "start_time": "2024-09-23T03:01:54.984841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeRF(torch.nn.Module):\n",
    "    def __init__(self, netdepth: int, netwidth: int, input_channel: int, output_channel: int, input_channel_views: int,\n",
    "                 use_viewdirs: bool,\n",
    "                 skips):\n",
    "        super(NeRF, self).__init__()\n",
    "        self.netdepth = netdepth\n",
    "        self.netwidth = netwidth\n",
    "        self.use_viewdirs = use_viewdirs\n",
    "        self.input_channel = input_channel\n",
    "        self.output_channel = output_channel\n",
    "        self.input_channel_views = input_channel_views\n",
    "        self.skips = skips\n",
    "\n",
    "        self.pts_linears = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(self.input_channel, self.netwidth)] + [\n",
    "                torch.nn.Linear(self.netwidth, self.netwidth) if i not in self.skips else torch.nn.Linear(\n",
    "                    self.netwidth + self.input_channel, self.netwidth) for i in range(self.netdepth - 1)])\n",
    "        self.views_linears = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(self.input_channel_views + self.netwidth, self.netwidth // 2)])\n",
    "        # self.views_linears = torch.nn.ModuleList(\n",
    "        #     [torch.nn.Linear(input_ch_views + self.netwidth, self.netwidth // 2)] + [\n",
    "        #         torch.nn.Linear(self.netwidth // 2, self.netwidth // 2) for i in range(self.netdepth // 2)])\n",
    "\n",
    "        if self.use_viewdirs:\n",
    "            self.feature_linear = torch.nn.Linear(self.netwidth, self.netwidth)\n",
    "            self.alpha_linear = torch.nn.Linear(self.netwidth, 1)\n",
    "            self.rgb_linear = torch.nn.Linear(self.netwidth // 2, 3)\n",
    "        else:\n",
    "            self.output_linear = torch.nn.Linear(self.netwidth, self.output_channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_pts, input_views = torch.split(x, [self.input_channel, self.input_channel_views], dim=-1)\n",
    "        h = input_pts\n",
    "        for i, l in enumerate(self.pts_linears):\n",
    "            h = self.pts_linears[i](h)\n",
    "            h = torch.nn.functional.relu(h)\n",
    "            if i in self.skips:\n",
    "                h = torch.cat([input_pts, h], -1)\n",
    "\n",
    "        if self.use_viewdirs:\n",
    "            alpha = self.alpha_linear(h)\n",
    "            feature = self.feature_linear(h)\n",
    "            h = torch.cat([feature, input_views], -1)\n",
    "\n",
    "            for i, l in enumerate(self.views_linears):\n",
    "                h = self.views_linears[i](h)\n",
    "                h = torch.nn.functional.relu(h)\n",
    "\n",
    "            rgb = self.rgb_linear(h)\n",
    "            outputs = torch.cat([rgb, alpha], -1)\n",
    "        else:\n",
    "            outputs = self.output_linear(h)\n",
    "\n",
    "        return outputs"
   ],
   "id": "d6af8c536b3a16db",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T03:01:55.153777Z",
     "start_time": "2024-09-23T03:01:55.143492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embed_fn = torch.nn.Identity()\n",
    "input_ch = 3\n",
    "if args.i_embed == 0:\n",
    "    embed_fn, input_ch = get_embedder(args.multires)\n",
    "\n",
    "embeddirs_fn = None\n",
    "input_ch_views = 0\n",
    "if args.use_viewdirs:\n",
    "    embeddirs_fn, input_ch_views = get_embedder(args.multires_views)\n",
    "\n",
    "output_ch = 5 if args.N_importance > 0 else 4\n",
    "\n",
    "model_coarse = NeRF(netdepth=args.netdepth, netwidth=args.netwidth, input_channel=input_ch, output_channel=output_ch,\n",
    "                    input_channel_views=input_ch_views, use_viewdirs=args.use_viewdirs, skips=[4]).to(device)\n",
    "grad_vars_coarse = list(model_coarse.parameters())\n",
    "model_fine = NeRF(netdepth=args.netdepth_fine, netwidth=args.netwidth_fine, input_channel=input_ch,\n",
    "                  output_channel=output_ch, input_channel_views=input_ch_views, use_viewdirs=args.use_viewdirs,\n",
    "                  skips=[4]).to(device)\n",
    "grad_vars_fine = list(model_coarse.parameters()) + list(model_fine.parameters())\n",
    "optimizer = torch.optim.Adam(params=grad_vars_fine, lr=args.lrate, betas=(0.9, 0.999))\n",
    "\n",
    "network_query_fn = lambda inputs, viewdirs, network_fn: run_network(inputs, viewdirs, network_fn,\n",
    "                                                                    embed_fn=embed_fn,\n",
    "                                                                    embeddirs_fn=embeddirs_fn,\n",
    "                                                                    netchunk=args.netchunk)"
   ],
   "id": "24a1f4f011923f9c",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T03:01:55.329811Z",
     "start_time": "2024-09-23T03:01:55.307872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# for i in trange(1, 2):\n",
    "image_index = np.random.choice(i_train)\n",
    "target_image = torch.tensor(images_concatenated[image_index])\n",
    "target_pose = torch.tensor(poses_concatenated[image_index])\n",
    "rays_o, rays_d = get_rays(height, width, K, torch.tensor(poses_concatenated[0]))\n",
    "\n",
    "coords = torch.reshape(torch.stack(\n",
    "    torch.meshgrid(torch.linspace(0, height - 1, height), torch.linspace(0, width - 1, width), indexing='ij'), -1),\n",
    "    [-1, 2])\n",
    "select_coords = coords[np.random.choice(coords.shape[0], size=[args.N_rand], replace=False)].long()\n",
    "select_rays_o = rays_o[select_coords[:, 0], select_coords[:, 1]]\n",
    "select_rays_d = rays_d[select_coords[:, 0], select_coords[:, 1]]\n",
    "target_s = target_image[select_coords[:, 0], select_coords[:, 1]]\n",
    "\n",
    "viewdirs = select_rays_d\n",
    "viewdirs = viewdirs / torch.norm(viewdirs, dim=-1, keepdim=True)\n",
    "viewdirs = torch.reshape(viewdirs, [-1, 3]).float()\n",
    "\n",
    "rays_final = torch.cat([select_rays_o, select_rays_d, near * torch.ones_like(select_rays_d[..., :1]),\n",
    "                        far * torch.ones_like(select_rays_d[..., :1]), viewdirs], -1)\n",
    "\n",
    "print(rays_final.shape)"
   ],
   "id": "ecbb420cb21552f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 11])\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T03:25:05.659123Z",
     "start_time": "2024-09-23T03:25:05.645798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N_rays = rays_final.shape[0]\n",
    "rays_o, rays_d = rays_final[:,0:3], rays_final[:,3:6]\n",
    "viewdirs = rays_final[:,-3:] if rays_final.shape[-1] > 8 else None\n",
    "bounds = torch.reshape(rays_final[...,6:8], [-1,1,2])\n",
    "near, far = bounds[...,0], bounds[...,1]\n",
    "print(near[0, 0], far[0, 0])"
   ],
   "id": "647c226b088294a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., device='cuda:0') tensor(6., device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T03:26:53.701135Z",
     "start_time": "2024-09-23T03:26:53.684726Z"
    }
   },
   "cell_type": "code",
   "source": "torch.linspace(0., 1., steps=args.N_samples)",
   "id": "53252763540a6f78",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0159, 0.0317, 0.0476, 0.0635, 0.0794, 0.0952, 0.1111, 0.1270,\n",
       "        0.1429, 0.1587, 0.1746, 0.1905, 0.2063, 0.2222, 0.2381, 0.2540, 0.2698,\n",
       "        0.2857, 0.3016, 0.3175, 0.3333, 0.3492, 0.3651, 0.3810, 0.3968, 0.4127,\n",
       "        0.4286, 0.4444, 0.4603, 0.4762, 0.4921, 0.5079, 0.5238, 0.5397, 0.5556,\n",
       "        0.5714, 0.5873, 0.6032, 0.6190, 0.6349, 0.6508, 0.6667, 0.6825, 0.6984,\n",
       "        0.7143, 0.7302, 0.7460, 0.7619, 0.7778, 0.7937, 0.8095, 0.8254, 0.8413,\n",
       "        0.8571, 0.8730, 0.8889, 0.9048, 0.9206, 0.9365, 0.9524, 0.9683, 0.9841,\n",
       "        1.0000], device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
