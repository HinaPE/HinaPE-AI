{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# NeRF",
   "id": "8255232b5eef9537"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0. Hyperparameters",
   "id": "4980771534bb1669"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T09:16:38.729760Z",
     "start_time": "2024-09-24T09:16:38.726056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import imageio.v2 as imageio\n",
    "import cv2\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "args = {\n",
    "    # 1. Model\n",
    "    \"coarse_net_width\": 256,\n",
    "    \"coarse_net_depth\": 8,\n",
    "    \"fine_net_width\": 256,\n",
    "    \"fine_net_depth\": 8,\n",
    "    \"learning_rate\": 5e-4,\n",
    "\n",
    "    # 2. Position Encoding\n",
    "    'datadir': './data/nerf_synthetic/lego',\n",
    "}\n",
    "args = argparse.Namespace(**args)"
   ],
   "id": "61c519a77fec105a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Load Data and Preprocess",
   "id": "bf749b3ae96e8e8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T09:16:38.745061Z",
     "start_time": "2024-09-24T09:16:38.738805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data_blender(in_type: str, in_skip: int = 1) -> Tuple[np.ndarray, np.ndarray, int, float]:\n",
    "    with open(os.path.normpath(os.path.join(args.datadir, 'transforms_{}.json'.format(in_type))), 'r') as fp:\n",
    "        _meta = json.load(fp)\n",
    "        _image_array = []\n",
    "        _pose_array = []\n",
    "        for _frame in _meta['frames'][::in_skip]:\n",
    "            _image_array.append(\n",
    "                imageio.imread(os.path.normpath(os.path.join(args.datadir, _frame['file_path'] + '.png'))))\n",
    "            _pose_array.append(np.array(_frame['transform_matrix']))\n",
    "        return (np.array(_image_array) / 255.).astype(np.float32), np.array(_pose_array), len(_meta['frames']), _meta[\n",
    "            'camera_angle_x']\n",
    "\n",
    "\n",
    "def resample_images(in_origin: np.ndarray, in_focal, in_rate: float) -> Tuple[np.ndarray, int, int, float]:\n",
    "    _target_height, _target_width, _target_focal = int(in_origin.shape[1] * in_rate), int(\n",
    "        in_origin.shape[2] * in_rate), in_focal * in_rate\n",
    "    _ret = np.zeros((in_origin.shape[0], _target_height, _target_width, 4))\n",
    "    for _idx, _image in enumerate(in_origin):\n",
    "        _ret[_idx] = cv2.resize(_image, (_target_width, _target_height), interpolation=cv2.INTER_AREA)\n",
    "    return _ret, _target_width, _target_height, _target_focal\n",
    "\n",
    "\n",
    "def make_white_background(in_origin: np.ndarray) -> np.ndarray:\n",
    "    return in_origin[..., :3] * in_origin[..., -1:] + (1. - in_origin[..., -1:])"
   ],
   "id": "ec066f984345a849",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Model",
   "id": "480f252ce4c39f69"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-24T09:16:38.791401Z",
     "start_time": "2024-09-24T09:16:38.786524Z"
    }
   },
   "source": [
    "class NeRF(torch.nn.Module):\n",
    "    def __init__(self, net_width: int, net_depth: int, input_channel: int, output_channel: int,\n",
    "                 input_channel_views: int, use_view: bool, skips: list):\n",
    "        super(NeRF, self).__init__()\n",
    "        self.net_width = net_width\n",
    "        self.net_depth = net_depth\n",
    "        self.input_channel = input_channel\n",
    "        self.output_channel = output_channel\n",
    "        self.input_channel_views = input_channel_views\n",
    "        self.use_view = use_view\n",
    "        self.skips = skips\n",
    "\n",
    "        self.points_layers = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(in_features=input_channel, out_features=net_width)] + [\n",
    "                torch.nn.Linear(in_features=net_width, out_features=net_width) if i not in skips else torch.nn.Linear(\n",
    "                    in_features=(net_width + input_channel), out_features=net_width) for i in range(net_depth - 1)])\n",
    "        self.views_layers = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(in_features=input_channel_views + net_width, out_features=net_width // 2)])\n",
    "        if use_view:\n",
    "            self.feature_layer = torch.nn.Linear(net_width, net_width)\n",
    "            self.alpha_layer = torch.nn.Linear(net_width, 1)\n",
    "            self.rgb_layer = torch.nn.Linear(net_width // 2, 3)\n",
    "        else:\n",
    "            self.output_linear = torch.nn.Linear(net_width, output_channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Position Encoding",
   "id": "2ff8f6a8decbd1ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T09:16:38.847380Z",
     "start_time": "2024-09-24T09:16:38.839440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_rays(in_width: int, in_height: int, in_focal: float, in_pose: torch.Tensor) -> Tuple[\n",
    "    torch.Tensor, torch.Tensor]:\n",
    "    _x, _y = torch.meshgrid(torch.linspace(0, in_width - 1, in_width), torch.linspace(0, in_height - 1, in_height),\n",
    "                            indexing='xy')\n",
    "    _rays_dirs = torch.matmul(\n",
    "        torch.stack([(_x - in_width / 2.) / in_focal, -(_y - in_height / 2.) / in_focal, -torch.ones_like(_x)], -1),\n",
    "        in_pose[0:3, 0:3].T)  # seems no need to normalize\n",
    "    _rays_oris = in_pose[0:3, -1].expand(_rays_dirs.shape)\n",
    "    return _rays_oris, _rays_dirs\n",
    "\n",
    "\n",
    "def generate_query_points(in_rays_oris: torch.Tensor, in_rays_dirs: torch.Tensor, in_near: float, in_far: float,\n",
    "                          in_batch_size: int, in_sample_size: int) -> torch.Tensor:\n",
    "    _height, _width = in_rays_oris.shape[0], in_rays_oris.shape[1]\n",
    "    _coords = torch.reshape(torch.stack(\n",
    "        torch.meshgrid(torch.linspace(0, _height - 1, _height), torch.linspace(0, _width - 1, _width),\n",
    "                       indexing='ij'), -1), [-1, 2])\n",
    "    _selected_coords = _coords[np.random.choice(_coords.shape[0], size=[in_batch_size], replace=False)].long()\n",
    "    _selected_rays_oris = in_rays_oris[_selected_coords[:, 0], _selected_coords[:, 1]]\n",
    "    _selected_rays_dirs = in_rays_dirs[_selected_coords[:, 0], _selected_coords[:, 1]]\n",
    "    _t_vals = torch.linspace(0., 1., steps=in_sample_size)\n",
    "    _z_vals = 1. / (1. / in_near * (1. - _t_vals) + 1. / in_far * _t_vals)\n",
    "    _z_vals = _z_vals.expand([in_batch_size, in_sample_size])\n",
    "\n",
    "    if True:  # perturb samples\n",
    "        _mid = .5 * (_z_vals[..., 1:] + _z_vals[..., :-1])\n",
    "        _upper = torch.cat([_mid, _z_vals[..., -1:]], -1)\n",
    "        _lower = torch.cat([_z_vals[..., :1], _mid], -1)\n",
    "        _t_rand = torch.rand(_z_vals.shape)\n",
    "        _z_vals = _lower + (_upper - _lower) * _t_rand\n",
    "\n",
    "    _sampled_points = _selected_rays_oris[..., None, :] + _selected_rays_dirs[..., None, :] * _z_vals[..., :, None]\n",
    "    return _sampled_points\n",
    "\n",
    "\n",
    "def generate_position_encoding_fn(in_multires: int):\n",
    "    _input_dims = 3\n",
    "    _out_dim = _input_dims\n",
    "    _max_freq_log2 = in_multires - 1\n",
    "    _num_freq = in_multires\n",
    "    _freq_bands = 2. ** torch.linspace(0., _max_freq_log2, steps=_num_freq)\n",
    "\n",
    "    embed_fns = [lambda x: x]\n",
    "    for _freq in _freq_bands:\n",
    "        for _p_fn in [torch.sin, torch.cos]:\n",
    "            embed_fns.append(lambda x, p_fn=_p_fn, freq=_freq: p_fn(x * freq))\n",
    "            _out_dim += _input_dims\n",
    "\n",
    "    _embed_fn = lambda x: torch.cat([fn(x) for fn in embed_fns], -1)\n",
    "    return _embed_fn, _out_dim"
   ],
   "id": "b500ade41f20b6fe",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Volume Rendering",
   "id": "e3f1ba44c93b921"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T09:27:07.670619Z",
     "start_time": "2024-09-24T09:27:07.667006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def render(in_points: torch.Tensor):\n",
    "    pass"
   ],
   "id": "e16a076229324463",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Training",
   "id": "6acbca8eae379468"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T09:22:36.310568Z",
     "start_time": "2024-09-24T09:22:36.307351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train():\n",
    "    pass"
   ],
   "id": "23c2349c84068011",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T09:16:45.790519Z",
     "start_time": "2024-09-24T09:16:38.888387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "IMAGES_TRAIN, POSE_TRAIN, FRAMES_TRAIN, ANGLE_TRAIN = load_data_blender('train', 1)\n",
    "IMAGES_VAL, POSE_VAL, FRAMES_VAL, ANGLE_VAL = load_data_blender('val', 8)\n",
    "IMAGES_TEST, POSE_TEST, FRAMES_TEST, ANGLE_TEST = load_data_blender('test', 8)\n",
    "\n",
    "WIDTH_ORIGIN, HEIGHT_ORIGIN = IMAGES_TRAIN.shape[2], IMAGES_TRAIN.shape[1]\n",
    "FOCAL_ORIGIN = 0.5 * WIDTH_ORIGIN / np.tan(0.5 * ANGLE_TRAIN)\n",
    "\n",
    "IMAGES_TRAIN_RESAMPLED, WIDTH_TRAIN_RESAMPLED, HEIGHT_TRAIN_RESAMPLED, FOCAL_TRAIN_RESAMPLED = \\\n",
    "    resample_images(IMAGES_TRAIN, FOCAL_ORIGIN, 0.5)\n",
    "IMAGES_VAL_RESAMPLED, WIDTH_VAL_RESAMPLED, HEIGHT_VAL_RESAMPLED, FOCAL_VAL_RESAMPLED = \\\n",
    "    resample_images(IMAGES_VAL, FOCAL_ORIGIN, 0.5)\n",
    "IMAGES_TEST_RESAMPLED, WIDTH_TEST_RESAMPLED, HEIGHT_TEST_RESAMPLED, FOCAL_TEST_RESAMPLED = \\\n",
    "    resample_images(IMAGES_TEST, FOCAL_ORIGIN, 0.5)\n",
    "\n",
    "IMAGES_TRAIN_WHITE_BACKGROUND = make_white_background(IMAGES_TRAIN)\n",
    "IMAGES_VAL_WHITE_BACKGROUND = make_white_background(IMAGES_VAL)\n",
    "IMAGES_TEST_WHITE_BACKGROUND = make_white_background(IMAGES_TEST)\n",
    "\n",
    "MODEL_COARSE = NeRF(args.coarse_net_width, args.coarse_net_depth, 3, 4, 8, True, [4])\n",
    "GRAD_VARS = list(MODEL_COARSE.parameters())\n",
    "OPTIMIZER = torch.optim.Adam(params=GRAD_VARS, lr=args.learning_rate, betas=(0.9, 0.999))\n",
    "\n",
    "RAY_ORIS, RAY_DIRS = generate_rays(WIDTH_ORIGIN, HEIGHT_ORIGIN, FOCAL_ORIGIN, torch.tensor(POSE_TRAIN[0]).float())\n",
    "POINTS = generate_query_points(RAY_ORIS, RAY_DIRS, 2., 6., 1024, 64)\n",
    "POINTS_FLAT = torch.reshape(POINTS, [-1, POINTS.shape[-1]])\n",
    "EMBED_fn, OUT_DIM = generate_position_encoding_fn(10)"
   ],
   "id": "83db28c9da27b08c",
   "outputs": [],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
