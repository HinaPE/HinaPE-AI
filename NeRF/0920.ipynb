{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# NeRF",
   "id": "8255232b5eef9537"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0. Hyperparameters",
   "id": "4980771534bb1669"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:05:10.352106Z",
     "start_time": "2024-09-25T06:05:08.700339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import imageio.v2 as imageio\n",
    "import cv2\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "args = {\n",
    "    # 1. Model\n",
    "    \"coarse_net_width\": 256,\n",
    "    \"coarse_net_depth\": 8,\n",
    "    \"fine_net_width\": 256,\n",
    "    \"fine_net_depth\": 8,\n",
    "\n",
    "    # 2. Others\n",
    "    'datadir': './data/nerf_synthetic/lego',\n",
    "    'resample_rate': 0.5,\n",
    "    'lrate': 0.0005,\n",
    "    'lrate_decay': 500,\n",
    "}\n",
    "args = argparse.Namespace(**args)\n",
    "torch.set_default_device('cuda')\n",
    "torch.set_default_dtype(torch.float32)"
   ],
   "id": "61c519a77fec105a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Load Data and Preprocess",
   "id": "bf749b3ae96e8e8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:05:10.361810Z",
     "start_time": "2024-09-25T06:05:10.354571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data_blender(in_type: str, in_skip: int = 1) -> Tuple[np.ndarray, np.ndarray, int, float]:\n",
    "    with open(os.path.normpath(os.path.join(args.datadir, 'transforms_{}.json'.format(in_type))), 'r') as fp:\n",
    "        _meta = json.load(fp)\n",
    "        _image_array = []\n",
    "        _pose_array = []\n",
    "        for _frame in _meta['frames'][::in_skip]:\n",
    "            _image_array.append(\n",
    "                imageio.imread(os.path.normpath(os.path.join(args.datadir, _frame['file_path'] + '.png'))))\n",
    "            _pose_array.append(np.array(_frame['transform_matrix']))\n",
    "        _images = (np.array(_image_array) / 255.).astype(np.float32)\n",
    "        _poses = np.array(_pose_array).astype(np.float32)\n",
    "        _frames = len(_image_array)\n",
    "        _angle = _meta['camera_angle_x']\n",
    "        return _images, _poses, _frames, _angle\n",
    "\n",
    "\n",
    "def resample_images(in_origin: np.ndarray, in_focal, in_rate: float) -> Tuple[np.ndarray, int, int, float]:\n",
    "    _target_height, _target_width, _target_focal = int(in_origin.shape[1] * in_rate), int(\n",
    "        in_origin.shape[2] * in_rate), in_focal * in_rate\n",
    "    _ret = np.zeros((in_origin.shape[0], _target_height, _target_width, 4))\n",
    "    for _idx, _image in enumerate(in_origin):\n",
    "        _ret[_idx] = cv2.resize(_image, (_target_width, _target_height), interpolation=cv2.INTER_AREA)\n",
    "    return _ret, _target_width, _target_height, _target_focal\n",
    "\n",
    "\n",
    "def make_white_background(in_origin: np.ndarray) -> np.ndarray:\n",
    "    return in_origin[..., :3] * in_origin[..., -1:] + (1. - in_origin[..., -1:])"
   ],
   "id": "ec066f984345a849",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Model",
   "id": "480f252ce4c39f69"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-25T06:05:10.784068Z",
     "start_time": "2024-09-25T06:05:10.777286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeRF(torch.nn.Module):\n",
    "    def __init__(self, net_width: int, net_depth: int, input_channel: int, input_channel_views: int, skips: list):\n",
    "        super(NeRF, self).__init__()\n",
    "        self.skips = skips\n",
    "\n",
    "        self.points_layers = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(in_features=input_channel, out_features=net_width)] + [\n",
    "                torch.nn.Linear(in_features=net_width, out_features=net_width) if i not in skips else torch.nn.Linear(\n",
    "                    in_features=(net_width + input_channel), out_features=net_width) for i in range(net_depth - 1)])\n",
    "        self.views_layers = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(in_features=input_channel_views + net_width, out_features=net_width // 2)])\n",
    "\n",
    "        self.feature_layer = torch.nn.Linear(net_width, net_width)\n",
    "        self.alpha_layer = torch.nn.Linear(net_width, 1)\n",
    "        self.rgb_layer = torch.nn.Linear(net_width // 2, 3)\n",
    "\n",
    "    def forward(self, points: torch.Tensor, views: torch.Tensor):\n",
    "        h = points\n",
    "        for i, l in enumerate(self.points_layers):\n",
    "            h = self.points_layers[i](h)\n",
    "            h = torch.nn.functional.relu(h)\n",
    "            if i in self.skips:\n",
    "                h = torch.cat([points, h], -1)\n",
    "\n",
    "        alpha = self.alpha_layer(h)\n",
    "        feature = self.feature_layer(h)\n",
    "        h = torch.cat([feature, views], -1)\n",
    "\n",
    "        for i, l in enumerate(self.views_layers):\n",
    "            h = self.views_layers[i](h)\n",
    "            h = torch.nn.functional.relu(h)\n",
    "\n",
    "        rgb = self.rgb_layer(h)\n",
    "        outputs = torch.cat([rgb, alpha], -1)\n",
    "        return outputs"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Position Encoding",
   "id": "2ff8f6a8decbd1ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:05:10.799256Z",
     "start_time": "2024-09-25T06:05:10.790099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_rays(in_width: int, in_height: int, in_focal: float, in_pose: torch.Tensor) -> Tuple[\n",
    "    torch.Tensor, torch.Tensor]:\n",
    "    _x, _y = torch.meshgrid(torch.linspace(0, in_width - 1, in_width), torch.linspace(0, in_height - 1, in_height),\n",
    "                            indexing='xy')\n",
    "    _rays_dirs = torch.matmul(\n",
    "        torch.stack([(_x - in_width / 2.) / in_focal, -(_y - in_height / 2.) / in_focal, -torch.ones_like(_x)], -1),\n",
    "        in_pose[0:3, 0:3].T)  # seems no need to normalize\n",
    "    _rays_oris = in_pose[0:3, -1].expand(_rays_dirs.shape)\n",
    "    return _rays_oris, _rays_dirs\n",
    "\n",
    "\n",
    "def generate_query_points(in_rays_oris: torch.Tensor, in_rays_dirs: torch.Tensor, in_near: float, in_far: float,\n",
    "                          in_batch_size: int, in_sample_size: int) -> Tuple[\n",
    "    torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    _height, _width = in_rays_oris.shape[0], in_rays_oris.shape[1]\n",
    "    _coords = torch.reshape(torch.stack(\n",
    "        torch.meshgrid(torch.linspace(0, _height - 1, _height), torch.linspace(0, _width - 1, _width),\n",
    "                       indexing='ij'), -1), [-1, 2])\n",
    "    _selected_coords = _coords[np.random.choice(_coords.shape[0], size=[in_batch_size], replace=False)].long()\n",
    "    _selected_rays_oris = in_rays_oris[_selected_coords[:, 0], _selected_coords[:, 1]]\n",
    "    _selected_rays_dirs = in_rays_dirs[_selected_coords[:, 0], _selected_coords[:, 1]]\n",
    "    _t_vals = torch.linspace(0., 1., steps=in_sample_size)\n",
    "    _z_vals = 1. / (1. / in_near * (1. - _t_vals) + 1. / in_far * _t_vals)\n",
    "    _z_vals = _z_vals.expand([in_batch_size, in_sample_size])\n",
    "\n",
    "    if True:  # perturb samples\n",
    "        _mid = .5 * (_z_vals[..., 1:] + _z_vals[..., :-1])\n",
    "        _upper = torch.cat([_mid, _z_vals[..., -1:]], -1)\n",
    "        _lower = torch.cat([_z_vals[..., :1], _mid], -1)\n",
    "        _t_rand = torch.rand(_z_vals.shape)\n",
    "        _z_vals = _lower + (_upper - _lower) * _t_rand\n",
    "\n",
    "    _sampled_points = _selected_rays_oris[..., None, :] + _selected_rays_dirs[..., None, :] * _z_vals[..., :, None]\n",
    "    _view_dirs = _selected_rays_dirs / torch.norm(_selected_rays_dirs, dim=-1, keepdim=True)\n",
    "    _view_dirs = _view_dirs[:, None].expand(_sampled_points.shape)\n",
    "    return _sampled_points, _view_dirs, _z_vals, _selected_rays_dirs, _selected_coords\n",
    "\n",
    "\n",
    "def generate_position_encoding_fn(in_multires: int):\n",
    "    _input_dims = 3\n",
    "    _out_dim = _input_dims\n",
    "    _max_freq_log2 = in_multires - 1\n",
    "    _num_freq = in_multires\n",
    "    _freq_bands = 2. ** torch.linspace(0., _max_freq_log2, steps=_num_freq)\n",
    "\n",
    "    embed_fns = [lambda x: x]\n",
    "    for _freq in _freq_bands:\n",
    "        for _p_fn in [torch.sin, torch.cos]:\n",
    "            embed_fns.append(lambda x, p_fn=_p_fn, freq=_freq: p_fn(x * freq))\n",
    "            _out_dim += _input_dims\n",
    "\n",
    "    _embed_fn = lambda x: torch.cat([fn(x) for fn in embed_fns], -1)\n",
    "    return _embed_fn, _out_dim"
   ],
   "id": "b500ade41f20b6fe",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Volume Rendering",
   "id": "e3f1ba44c93b921"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:05:10.810817Z",
     "start_time": "2024-09-25T06:05:10.805890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def render(in_raw: torch.Tensor, in_z_vals: torch.Tensor, in_rays_dirs: torch.Tensor) -> Tuple[\n",
    "    torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    raw2alpha = lambda raw, dists, act_fn=torch.nn.functional.relu: 1. - torch.exp(-act_fn(raw) * dists)\n",
    "\n",
    "    dists = in_z_vals[..., 1:] - in_z_vals[..., :-1]\n",
    "    dists = torch.cat([dists, torch.tensor([1e10]).expand(dists[..., :1].shape)], -1)  # [N_rays, N_samples]\n",
    "    dists = dists * torch.norm(in_rays_dirs[..., None, :], dim=-1)\n",
    "    rgb = torch.sigmoid(in_raw[..., :3])  # [N_rays, N_samples, 3]\n",
    "    alpha = raw2alpha(in_raw[..., 3], dists)  # [N_rays, N_samples]\n",
    "    weights = alpha * torch.cumprod(torch.cat([torch.ones((alpha.shape[0], 1)), 1. - alpha + 1e-10], -1), -1)[:, :-1]\n",
    "    rgb_map = torch.sum(weights[..., None] * rgb, -2)  # [N_rays, 3]\n",
    "    depth_map = torch.sum(weights * in_z_vals, -1)\n",
    "    disp_map = 1. / torch.max(1e-10 * torch.ones_like(depth_map), depth_map / torch.sum(weights, -1))\n",
    "    acc_map = torch.sum(weights, -1)\n",
    "    rgb_map = rgb_map + (1. - acc_map[..., None])\n",
    "    return rgb_map, disp_map, acc_map, weights, depth_map"
   ],
   "id": "e16a076229324463",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Training",
   "id": "6acbca8eae379468"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:05:17.902488Z",
     "start_time": "2024-09-25T06:05:10.815555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "IMAGES_TRAIN, POSE_TRAIN, FRAMES_TRAIN, ANGLE_TRAIN = load_data_blender('train', 1)\n",
    "IMAGES_VAL, POSE_VAL, FRAMES_VAL, ANGLE_VAL = load_data_blender('val', 8)\n",
    "IMAGES_TEST, POSE_TEST, FRAMES_TEST, ANGLE_TEST = load_data_blender('test', 8)\n",
    "WIDTH_ORIGIN, HEIGHT_ORIGIN = IMAGES_TRAIN.shape[2], IMAGES_TRAIN.shape[1]\n",
    "FOCAL_ORIGIN = 0.5 * WIDTH_ORIGIN / np.tan(0.5 * ANGLE_TRAIN)\n",
    "# IMAGES_TRAIN_RESAMPLED, WIDTH_TRAIN_RESAMPLED, HEIGHT_TRAIN_RESAMPLED, FOCAL_TRAIN_RESAMPLED = \\\n",
    "#     resample_images(IMAGES_TRAIN, FOCAL_ORIGIN, args.resample_rate)\n",
    "# IMAGES_VAL_RESAMPLED, WIDTH_VAL_RESAMPLED, HEIGHT_VAL_RESAMPLED, FOCAL_VAL_RESAMPLED = \\\n",
    "#     resample_images(IMAGES_VAL, FOCAL_ORIGIN, args.resample_rate)\n",
    "# IMAGES_TEST_RESAMPLED, WIDTH_TEST_RESAMPLED, HEIGHT_TEST_RESAMPLED, FOCAL_TEST_RESAMPLED = \\\n",
    "#     resample_images(IMAGES_TEST, FOCAL_ORIGIN, args.resample_rate)\n",
    "\n",
    "IMAGES_TRAIN_WHITE_BACKGROUND = make_white_background(IMAGES_TRAIN)\n",
    "IMAGES_VAL_WHITE_BACKGROUND = make_white_background(IMAGES_VAL)\n",
    "IMAGES_TEST_WHITE_BACKGROUND = make_white_background(IMAGES_TEST)\n",
    "\n",
    "EMBED_fn, OUT_DIM = generate_position_encoding_fn(10)\n",
    "EMBED_VIEW_fn, OUT_VIEW_DIM = generate_position_encoding_fn(4)\n",
    "MODEL_COARSE = NeRF(args.coarse_net_width, args.coarse_net_depth, OUT_DIM, OUT_VIEW_DIM, [4])\n",
    "GRAD_VARS = list(MODEL_COARSE.parameters())\n",
    "OPTIMIZER = torch.optim.Adam(params=GRAD_VARS, lr=args.lrate, betas=(0.9, 0.999))"
   ],
   "id": "83db28c9da27b08c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:05:18.260321Z",
     "start_time": "2024-09-25T06:05:18.015181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "IDX_TRAIN = np.arange(FRAMES_TRAIN)\n",
    "IDX_VAL = np.arange(FRAMES_VAL)\n",
    "IDX_TEST = np.arange(FRAMES_TEST)\n",
    "img2mse = lambda x, y: torch.mean((x - y) ** 2)\n",
    "mse2psnr = lambda x: -10. * torch.log(x) / torch.log(torch.tensor([10.]))\n",
    "for epoch in trange(1):\n",
    "    # 1. Forward\n",
    "    idx_image = np.random.choice(IDX_TRAIN)\n",
    "    target_image = torch.tensor(IMAGES_TRAIN_WHITE_BACKGROUND[idx_image]).float()\n",
    "    target_pose = torch.tensor(POSE_TRAIN[idx_image]).float()\n",
    "    ray_oris, ray_dirs = generate_rays(in_width=WIDTH_ORIGIN, in_height=HEIGHT_ORIGIN, in_focal=FOCAL_ORIGIN,\n",
    "                                       in_pose=target_pose)\n",
    "    points, view_dirs, z_vars, selected_rays_dirs, selected_coords = generate_query_points(in_rays_oris=ray_oris, in_rays_dirs=ray_dirs,\n",
    "                                                                          in_near=2.,\n",
    "                                                                          in_far=6.,\n",
    "                                                                          in_batch_size=1024, in_sample_size=64)\n",
    "    points_flat = torch.reshape(points, [-1, points.shape[-1]])\n",
    "    view_dirs_flat = torch.reshape(view_dirs, [-1, view_dirs.shape[-1]])\n",
    "    embedded_points = EMBED_fn(points_flat)\n",
    "    embedded_view_dirs = EMBED_VIEW_fn(view_dirs_flat)\n",
    "    raw_flat = MODEL_COARSE(embedded_points, embedded_view_dirs)\n",
    "    raw = torch.reshape(raw_flat, list(points.shape[:-1]) + [raw_flat.shape[-1]])\n",
    "\n",
    "    rgb_map, disp_map, acc_map, weights, depth_map = render(in_raw=raw, in_z_vals=z_vars,\n",
    "                                                            in_rays_dirs=selected_rays_dirs)\n",
    "\n",
    "    # 2. Optimization\n",
    "    OPTIMIZER.zero_grad()\n",
    "    target_s = target_image[selected_coords[:, 0], selected_coords[:, 1]]\n",
    "    img_loss = img2mse(rgb_map, target_s)\n",
    "    loss = img_loss\n",
    "    psnr = mse2psnr(img_loss)\n",
    "    loss.backward()\n",
    "    OPTIMIZER.step()\n",
    "\n",
    "    # NOTE: IMPORTANT!\n",
    "    ###   update learning rate   ###\n",
    "    decay_rate = 0.1\n",
    "    decay_steps = args.lrate_decay * 1000\n",
    "    new_lrate = args.lrate * (decay_rate ** (epoch + 1 / decay_steps))\n",
    "    for param_group in OPTIMIZER.param_groups:\n",
    "        param_group['lr'] = new_lrate"
   ],
   "id": "984059f0b425274e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.30it/s]\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
